{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83e359c6-c41e-464b-acd8-8ad847b2cd9d",
   "metadata": {},
   "source": [
    "1.Linear Regression:\n",
    "\n",
    "Purpose: Predicts a continuous dependent variable based on one or more independent variables.\n",
    "Output: Continuous values (e.g., predicting house prices).\n",
    "Model: The relationship between the dependent variable \n",
    "𝑦\n",
    "y and the independent variables \n",
    "𝑋\n",
    "X is modeled as \n",
    "𝑦\n",
    "=\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "𝑋\n",
    "1\n",
    "+\n",
    "𝛽\n",
    "2\n",
    "𝑋\n",
    "2\n",
    "+\n",
    "…\n",
    "+\n",
    "𝛽\n",
    "𝑝\n",
    "𝑋\n",
    "𝑝\n",
    "y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " X \n",
    "1\n",
    "​\n",
    " +β \n",
    "2\n",
    "​\n",
    " X \n",
    "2\n",
    "​\n",
    " +…+β \n",
    "p\n",
    "​\n",
    " X \n",
    "p\n",
    "​\n",
    " .\n",
    "Logistic Regression:\n",
    "\n",
    "Purpose: Predicts a binary outcome (0 or 1) based on one or more independent variables.\n",
    "Output: Probability values between 0 and 1, typically interpreted as the probability of the positive class.\n",
    "Model: Uses the logistic function to model the probability \n",
    "𝑃\n",
    "P that the dependent variable \n",
    "𝑦\n",
    "y equals 1:\n",
    "𝑃\n",
    "(\n",
    "𝑦\n",
    "=\n",
    "1\n",
    "∣\n",
    "𝑋\n",
    ")\n",
    "=\n",
    "1\n",
    "1\n",
    "+\n",
    "𝑒\n",
    "−\n",
    "(\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "𝑋\n",
    "1\n",
    "+\n",
    "𝛽\n",
    "2\n",
    "𝑋\n",
    "2\n",
    "+\n",
    "…\n",
    "+\n",
    "𝛽\n",
    "𝑝\n",
    "𝑋\n",
    "𝑝\n",
    ")\n",
    "P(y=1∣X)= \n",
    "1+e \n",
    "−(β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " X \n",
    "1\n",
    "​\n",
    " +β \n",
    "2\n",
    "​\n",
    " X \n",
    "2\n",
    "​\n",
    " +…+β \n",
    "p\n",
    "​\n",
    " X \n",
    "p\n",
    "​\n",
    " )\n",
    " \n",
    "1\n",
    "​\n",
    " \n",
    "Example Scenario for Logistic Regression:\n",
    "\n",
    "Scenario: Predicting whether a customer will churn (leave the service) or not.\n",
    "Reason: The outcome (churn or no churn) is binary, making logistic regression suitable for this type of classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8038fc2-43df-4b5f-a559-33acba9fda8d",
   "metadata": {},
   "source": [
    "2.Cost Function:\n",
    "\n",
    "Log-Loss (Logistic Loss) or Binary Cross-Entropy Loss:\n",
    "𝐽\n",
    "(\n",
    "𝛽\n",
    ")\n",
    "=\n",
    "−\n",
    "1\n",
    "𝑛\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "[\n",
    "𝑦\n",
    "𝑖\n",
    "log\n",
    "⁡\n",
    "(\n",
    "ℎ\n",
    "𝛽\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    ")\n",
    ")\n",
    "+\n",
    "(\n",
    "1\n",
    "−\n",
    "𝑦\n",
    "𝑖\n",
    ")\n",
    "log\n",
    "⁡\n",
    "(\n",
    "1\n",
    "−\n",
    "ℎ\n",
    "𝛽\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    ")\n",
    ")\n",
    "]\n",
    "J(β)=− \n",
    "n\n",
    "1\n",
    "​\n",
    "  \n",
    "i=1\n",
    "∑\n",
    "n\n",
    "​\n",
    " [y \n",
    "i\n",
    "​\n",
    " log(h \n",
    "β\n",
    "​\n",
    " (x \n",
    "i\n",
    "​\n",
    " ))+(1−y \n",
    "i\n",
    "​\n",
    " )log(1−h \n",
    "β\n",
    "​\n",
    " (x \n",
    "i\n",
    "​\n",
    " ))]\n",
    "Where \n",
    "ℎ\n",
    "𝛽\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    ")\n",
    "h \n",
    "β\n",
    "​\n",
    " (x \n",
    "i\n",
    "​\n",
    " ) is the predicted probability for instance \n",
    "𝑖\n",
    "i.\n",
    "Optimization:\n",
    "\n",
    "Gradient Descent: Iteratively adjusts the parameters \n",
    "𝛽\n",
    "β to minimize the cost function.\n",
    "Variants of Gradient Descent: Stochastic Gradient Descent (SGD), Mini-Batch Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b527a129-2cb7-45a7-a842-c534ca4693ed",
   "metadata": {},
   "source": [
    "3.Regularization:\n",
    "\n",
    "Purpose: Prevents overfitting by adding a penalty to the cost function based on the magnitude of the coefficients.\n",
    "Types:\n",
    "\n",
    "L1 Regularization (Lasso):\n",
    "𝐽\n",
    "(\n",
    "𝛽\n",
    ")\n",
    "=\n",
    "−\n",
    "1\n",
    "𝑛\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "[\n",
    "𝑦\n",
    "𝑖\n",
    "log\n",
    "⁡\n",
    "(\n",
    "ℎ\n",
    "𝛽\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    ")\n",
    ")\n",
    "+\n",
    "(\n",
    "1\n",
    "−\n",
    "𝑦\n",
    "𝑖\n",
    ")\n",
    "log\n",
    "⁡\n",
    "(\n",
    "1\n",
    "−\n",
    "ℎ\n",
    "𝛽\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    ")\n",
    ")\n",
    "]\n",
    "+\n",
    "𝜆\n",
    "∑\n",
    "𝑗\n",
    "=\n",
    "1\n",
    "𝑝\n",
    "∣\n",
    "𝛽\n",
    "𝑗\n",
    "∣\n",
    "J(β)=− \n",
    "n\n",
    "1\n",
    "​\n",
    "  \n",
    "i=1\n",
    "∑\n",
    "n\n",
    "​\n",
    " [y \n",
    "i\n",
    "​\n",
    " log(h \n",
    "β\n",
    "​\n",
    " (x \n",
    "i\n",
    "​\n",
    " ))+(1−y \n",
    "i\n",
    "​\n",
    " )log(1−h \n",
    "β\n",
    "​\n",
    " (x \n",
    "i\n",
    "​\n",
    " ))]+λ \n",
    "j=1\n",
    "∑\n",
    "p\n",
    "​\n",
    " ∣β \n",
    "j\n",
    "​\n",
    " ∣\n",
    "L2 Regularization (Ridge):\n",
    "𝐽\n",
    "(\n",
    "𝛽\n",
    ")\n",
    "=\n",
    "−\n",
    "1\n",
    "𝑛\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "[\n",
    "𝑦\n",
    "𝑖\n",
    "log\n",
    "⁡\n",
    "(\n",
    "ℎ\n",
    "𝛽\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    ")\n",
    ")\n",
    "+\n",
    "(\n",
    "1\n",
    "−\n",
    "𝑦\n",
    "𝑖\n",
    ")\n",
    "log\n",
    "⁡\n",
    "(\n",
    "1\n",
    "−\n",
    "ℎ\n",
    "𝛽\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    ")\n",
    ")\n",
    "]\n",
    "+\n",
    "𝜆\n",
    "∑\n",
    "𝑗\n",
    "=\n",
    "1\n",
    "𝑝\n",
    "𝛽\n",
    "𝑗\n",
    "2\n",
    "J(β)=− \n",
    "n\n",
    "1\n",
    "​\n",
    "  \n",
    "i=1\n",
    "∑\n",
    "n\n",
    "​\n",
    " [y \n",
    "i\n",
    "​\n",
    " log(h \n",
    "β\n",
    "​\n",
    " (x \n",
    "i\n",
    "​\n",
    " ))+(1−y \n",
    "i\n",
    "​\n",
    " )log(1−h \n",
    "β\n",
    "​\n",
    " (x \n",
    "i\n",
    "​\n",
    " ))]+λ \n",
    "j=1\n",
    "∑\n",
    "p\n",
    "​\n",
    " β \n",
    "j\n",
    "2\n",
    "​\n",
    " \n",
    "Elastic Net: Combines L1 and L2 penalties.\n",
    "Effect:\n",
    "\n",
    "Shrinks the coefficients, reducing the model's complexity and preventing it from fitting noise in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4039851-1b6e-4bb8-bbd2-76697178dd8a",
   "metadata": {},
   "source": [
    "4.ROC Curve:\n",
    "\n",
    "Receiver Operating Characteristic (ROC) Curve: Plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings.\n",
    "Usage:\n",
    "\n",
    "AUC (Area Under the Curve): A single metric to evaluate the model's performance, with 1 being perfect and 0.5 being no better than random guessing.\n",
    "Interpretation: Helps to understand the trade-off between sensitivity (recall) and specificity, and choose an optimal threshold for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c070852-9092-4c7a-a2fd-04248a0f812b",
   "metadata": {},
   "source": [
    "5.Common Techniques:\n",
    "\n",
    "Univariate Selection: Select features based on statistical tests (e.g., chi-square test, ANOVA).\n",
    "Recursive Feature Elimination (RFE): Iteratively remove the least significant features based on their importance.\n",
    "Lasso (L1 Regularization): Automatically performs feature selection by shrinking some coefficients to zero.\n",
    "Tree-Based Methods: Use feature importance scores from tree-based models (e.g., Random Forest) to select features.\n",
    "PCA (Principal Component Analysis): Reduce dimensionality by transforming features into a smaller number of uncorrelated components.\n",
    "Benefits:\n",
    "\n",
    "Improves Model Performance: Reduces overfitting by eliminating irrelevant or redundant features.\n",
    "Enhances Interpretability: Simplifies the model, making it easier to understand and interpret.\n",
    "Reduces Computation: Speeds up training and prediction times by reducing the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54342b96-ec6d-4a35-b4c0-1c1f8961f726",
   "metadata": {},
   "source": [
    "6.Strategies:\n",
    "\n",
    "Resampling Techniques:\n",
    "Oversampling: Increase the number of minority class samples (e.g., SMOTE).\n",
    "Undersampling: Decrease the number of majority class samples.\n",
    "Class Weights:\n",
    "Adjust the weights of classes in the loss function to give more importance to the minority class.\n",
    "Threshold Adjustment:\n",
    "Change the decision threshold to favor the minority class.\n",
    "Synthetic Data Generation:\n",
    "Use techniques like SMOTE to generate synthetic samples for the minority class.\n",
    "Anomaly Detection Models:\n",
    "Treat the minority class as anomalies and use anomaly detection techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6d0305-6b0a-4ef8-9eeb-dbcb690be778",
   "metadata": {},
   "source": [
    "7.Multicollinearity:\n",
    "\n",
    "Problem: High correlation among independent variables can lead to unstable coefficient estimates.\n",
    "Solution:\n",
    "Variance Inflation Factor (VIF): Identify and remove or combine highly correlated predictors.\n",
    "Regularization: Use Ridge (L2) or Elastic Net regularization to mitigate multicollinearity.\n",
    "Class Imbalance:\n",
    "\n",
    "Problem: Imbalanced datasets can lead to biased models.\n",
    "Solution: Use the strategies mentioned above (resampling, class weights, etc.).\n",
    "Outliers and Noise:\n",
    "\n",
    "Problem: Outliers can distort the model's decision boundary.\n",
    "Solution: Detect and remove or mitigate the impact of outliers.\n",
    "Model Specification:\n",
    "\n",
    "Problem: Incorrectly specified models (e.g., missing interaction terms) can lead to poor performance.\n",
    "Solution: Conduct thorough exploratory data analysis (EDA) and feature engineering to identify relevant interactions and non-linear relationships.\n",
    "Overfitting:\n",
    "\n",
    "Problem: The model may perform well on training data but poorly on unseen data.\n",
    "Solution: Use regularization, cross-validation, and limit the complexity of the model.\n",
    "By addressing these challenges, you can build a robust logistic regression model that generalizes well to new data and provides reliable predictions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819efc28-0974-45c8-9f58-61224d03fe26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
